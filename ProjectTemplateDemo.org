#+TITLE:ProjectTemplate Demo
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: beamer
#+LATEX_HEADER: \usepackage{verbatim}
#+LATEX_HEADER: \graphicspath{{./analysis/reports/}} 

# README
# This is an Emacs Orgmode document that contains the code base for the project
# Uses Emacs 24, see  http://kieranhealy.org/emacs-starter-kit.html
# to create the required files and directories please follow the steps below in order:
# 1 in Emacs enter C-c C-v t (this will tangle the code that initialises the project, some tangling will fail, ignore errors)
# 2 open the new file 'init.r' and evaluate the code to create.project('analysis')
# 3 enter C-c C-v t again to complete the tangling (possible now because project directories have been created)
# 4 open the new file 'analysis/go.r' and evaluate the code chunks, also do the NMMAPSliteEgGam.r
# 5 open the new file 'analysis/reports/letters.tex' and run LaTeX to produce the report (if the images to include in First and Second letter sections below do exist then set eval to yes and uncomment the include figures chunk)
# 6 in Emacs enter C-c C-e l (Lowercase L), accept the invitation to evaluate 2 chunks of R code that copy the plots to the root directory and include in the tex file

#+name:overview
#+begin_src R :session *R* :tangle init.r :exports none :eval no
  ####
  # MAKE SURE YOU HAVE THE CORE LIBS
  if (!require(ProjectTemplate)) install.packages('ProjectTemplate', repos='http://cran.csiro.au'); require(ProjectTemplate)
  if (!require(lubridate)) install.packages('lubridate', repos='http://cran.csiro.au'); require(lubridate)
  if (!require(reshape)) install.packages('reshape', repos='http://cran.csiro.au'); require(reshape)
  if (!require(plyr)) install.packages('plyr', repos='http://cran.csiro.au'); require(plyr)
  if (!require(ggplot2)) install.packages('ggplot2', repos='http://cran.csiro.au'); require(ggplot2)
  if(!require(mgcv)) install.packages('mgcv', repos='http://cran.csiro.au');require(mgcv);
  require(splines)
  if(!require(NMMAPSlite)) install.packages('NMMAPSlite', repos='http://cran.csiro.au');require(NMMAPSlite)
  rootdir <- getwd()  
#+end_src
* The Compendium concept
\section{The Compendium concept}
My goal is to develop data analysis projects along the lines of the Compendium concept of Gentleman and Temple Lang (2007) \cite{Gentleman2007}.
Compendia are dynamic documents containing text, code and data.
Transformations are applied to the compendium to view its various aspects.

- Code Extraction (Tangle): source code
- Export (Weave): LaTeX, HTML, etc
- Code Evaluation

I'm also following the orgmode technique of Schulte et al (2012) \cite{Schulte}

* The R code that produced this report

I support the philosophy of Reproducible Research http://www.sciencemag.org/content/334/6060/1226.full, and where possible I provide data and code in the statistical software R that will allow analyses to be reproduced.  This document is prepared automatically from the associated Emacs Orgmode file.  If you do not have access to the Orgmode file please contact me.
#+name:copyright
#+begin_src R :session *R* :tangle init.r :exports none :eval no
cat('
 #######################################################################
 ## The R code is free software; please cite this paper as the source.  
 ## Copyright 2012, Ivan C Hanigan <ivan.hanigan@gmail.com> 
 ## This program is free software; you can redistribute it and/or modify
 ## it under the terms of the GNU General Public License as published by
 ## the Free Software Foundation; either version 2 of the License, or
 ## (at your option) any later version.
 ## 
 ## This program is distributed in the hope that it will be useful,
 ## but WITHOUT ANY WARRANTY; without even the implied warranty of
 ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 ## GNU General Public License for more details.
 ## Free Software
 ## Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
 ## 02110-1301, USA
 #######################################################################
')
#+end_src
* ProjectTemplate
\section{ProjectTemplate}
This is a simple demo of the R package \emph{ProjectTemplate} http://projecttemplate.net/ which is aimed at standardising the structure and general development of data analysis projects in R. 
A primary aim is to allow analysts to quickly get a project loaded up and ready to:
- reproduce or 
- create new data analyses.


* Why?
It has been recognised on the R blogosphere that it 
- is ``meant to handle very complex research projects'' (http://bryer.org/2012/maker-an-r-package-for-managing-document-building-and-versioning) and 
- is considered as being amongst the best approaches to the workflow for doing data analysis with R (http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html)

* The Reichian load, clean, func, do approach
\section{The Reichian load, clean, func, do approach}

The already mentioned blog post http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html also links to another`best' approach, the: 

- \emph{Reichian load, clean, func, do} approach http://stackoverflow.com/a/1434424.  


By Josh Reich.  I've also followed to prepare this demo using the tutorial and data from the package website http://projecttemplate.net/getting_started.html

* The Peng NMMAPSlite approach
\section{The Peng NMMAPSlite approach}
The other approach I followed was that of Roger Peng from Johns Hopkins and his NMMAPSlite R package \cite{Peng2004}.  Especially the function
\begin{verbatim}
readCity(name, collapseAge = FALSE, asDataFrame = TRUE)
\end{verbatim}

Arguments
- name 	character, abbreviated name of a city
- collapseAge 	logical, should age categories be collapsed?
- asDataFrame 	logical, should a data frame be returned?)

Description: Provides remote access to daily mortality, weather, and
        air pollution data from the National Morbidity, Mortality, and
        Air Pollution Study for 108 U.S. cities (1987--2000); data are
        obtained from the Internet-based Health and Air Pollution
        Surveillance System (iHAPSS)
* Init the project
\section{Init the project}
First we want to initialise the project directory.
#+name:init
#+begin_src R :session *R* :tangle init.r :eval no
  ####
  # init
  require('ProjectTemplate')
  create.project('analysis',minimal=TRUE)
#+end_src

* dir()
#+name:dir
#+begin_src R :session *R* :tangle init.r :eval no
  ####
  # init dir
  dir('analysis')
#+end_src

| cache   |
| config  |
| data    |
| munge   |
| README  |
| src     |
* The reports directory
  I've added the reports directory manually and asked the package author if this is generic enough to be in the defaults for 
\begin{verbatim} 
minimal = TRUE 
\end{verbatim}

I believe it may be as the \emph{Getting Started} guidebook states:
#+begin_quote
`It's meant to contain the sort of written descriptions of the results of your analyses that you'd \textbf{publish in a scientific paper.}

With that report written ..., we've gone through \textbf{the simplest sort of analysis you might run with ProjectTemplate}. 
#+end_quote
#+name:reports
#+begin_src R :session *R* :tangle init.r :eval no
  ####
  # init reports
  dir.create('analysis/reports')
#+end_src
* Do the analysis
\section{Do the analysis: use load,clean,func,do}
#+name:get tutorial data
#+begin_src R :session *R* :tangle analysis/go.r :eval no
  ####
  # this is the start of the analysis, 
  # assumes the init.r file has been run
  if(file.exists('analysis')) setwd('analysis')  
  Sys.Date()
  # keep a track of the dates the analysis is rerun
  getwd()
  # may want to keep a reference of the directory 
  # the project is in so we can track the history 
#+end_src

* Get the projecttemplate tutorial data
Get the data from [[http://projecttemplate.net/letters.csv.bz2]] (I downloaded on 13-4-2012)
Put it in the data directory for auto loading.

#+name:get tutorial data
#+begin_src R :session *R* :tangle analysis/go.r :eval no
  ####
  # analysis get tutorial data
  download.file('http://projecttemplate.net/letters.csv.bz2', 
    destfile = 'data/letters.csv.bz2', mode = 'wb')
  
#+end_src
* Tools
# \section{func}
Edit the \emph{config/global.dcf} file to make sure that the load\_libraries setting is turned on

#+name:analysis tools
#+begin_src dcf :tangle analysis/config/global.dcf :exports none :eval no
data_loading: on
munging: off
logging: off
load_libraries: on
libraries: reshape, plyr, ggplot2, stringr, lubridate, splines, mgcv, NMMAPSlite
as_factors: on
data_tables: off
#+end_src
* Load the analysis data
#\section{load}
#+name:analysis load
#+begin_src R :session *R* :tangle analysis/go.r :eval no
  ####
  # analysis load
  require(ProjectTemplate)
  load.project()
#+end_src
* check the analysis data
#\section{clean}
#+name:check letters
#+begin_src R :session *R* :tangle analysis/go.r :eval no
tail(letters)
#+end_src

| zyryan     | z | y |
| zythem     | z | y |
| zythia     | z | y |
| zythum     | z | y |
| zyzomys    | z | y |
| zyzzogeton | z | y |


* Develop munge code
#\section{load with processing (munge)}

Edit the \emph{munge/01-A.R} script so that it contains the following two lines of code:
#+name:edit munge
#+begin_src R :session *R* :tangle analysis/munge/01-A.r :eval no
# For our current analysis, we're interested in the total 
# number of occurrences of each letter in the first and 
# second letter positions and not in the words themselves.
# compute aggregates
first.letter.counts <- ddply(letters, c('FirstLetter'), 
  nrow)
second.letter.counts <- ddply(letters, c('SecondLetter'), 
  nrow)
#+end_src
Now if we run with 
\begin{verbatim}
load.project()
\end{verbatim}
all munging will happen automatically.  However...
# NB deprecated, it is better to do this with source('munge/xyz.r'); cache('xyz.r')??
#+name:load with munge
#+begin_src R :session *R* :exports none :eval no
  load.project()
  ls()
#+end_src
* To munge or not to munge?
As you'll see on the website, once the data munging is completed and outputs cached, load.project() will keep recomputing work over and over.  
The author suggests we manually edit our configuration file.
#+name:load without munge
#+begin_src R :session *R* :tangle analysis/go.r :eval no
 # edit the config file and turn munge on
 # load.project()
 # edit the config file and turn munge off
 # or my preference
 source('munge/01-A.r')
# which can be included in our first analysis script
# but subsequent analysis scripts can just call load.project() 
# without touching the config file
#+end_src
* Cache
Once munging is complete we cache the results
#+name:cache
#+begin_src R :session *R* :tangle analysis/go.r :eval no
  cache('first.letter.counts')
  cache('second.letter.counts')
#+end_src
# And need to keep an eye on the implications for our config file to avoid re-calculating these next time we 
#\begin{verbatim}
#load.project()
#\end{verbatim}


#\section{do}
* Plot first and second letter counts
Produce some simple density plots to see the shape of the first and second letter counts. 
- Create \emph{src/generate\_plots.R}. Use the src directory to store any analyses that you run. 
- The convention is that every analysis script starts with load.project() and then goes on to do something original with the data.

* Do generate plots
Write the first analysis script into a file in \textbf{src}
#+name:generate_plots
#+begin_src R :session *R* :tangle analysis/src/generate_plots.r :eval no
  require('ProjectTemplate')
  load.project()
  plot1 <- ggplot(first.letter.counts, aes(x = V1)) + 
    geom_density()
  ggsave(file.path('reports', 'plot1.pdf'))
  
  plot2 <- ggplot(second.letter.counts, aes(x = V1)) + 
    geom_density()
  ggsave(file.path('reports', 'plot2.pdf'))
  dev.off()
#+end_src

And now run it (I do this from a main `overview' script).

#+name:Do generate plots
#+begin_src R :session *R* :tangle analysis/go.r :eval no
  source('src/generate_plots.r')
#+end_src
* First letter

# \begin{figure}[!h]
# \centering
# \includegraphics[width=.6\textwidth]{plot1.pdf}
# \caption{plot1.pdf}
# \label{fig:plot1.pdf}
# \end{figure}

* Second letter

# \begin{figure}[!h]
# \centering
# \includegraphics[width=.6\textwidth]{plot2.pdf}
# \caption{plot2.pdf}
# \label{fig:plot2.pdf}
# \end{figure}

* Report results
\section{Report results}
We see that both the first and second letter distributions are very skewed. To make a note of this for posterity, we can write up our discovery in a text file that we store in the reports directory.
\begin{verbatim}

\documentclass[a4paper]{article}
\title{Letters analysis}
\author{Ivan Hanigan}
\begin{document}
\maketitle
blah blah blah
\end{document}

\end{verbatim}

#+name:report results
#+begin_src latex :session *R* :tangle analysis/reports/letters.tex :exports none :eval no
% Created 2012-03-29 Thu 12:41
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{verbatim}
\setlength{\parindent}{0in}
\providecommand{\alert}[1]{\textbf{#1}}

\title{Letters analysis}
\author{Ivan Hanigan}
\date{\today}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.8.03}}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\vspace*{1cm}
\hrule

\section{Intro}
\clearpage
\section{First letter}
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{plot1.pdf}
\caption{plot1.pdf}
\label{fig:plot1.pdf}
\end{figure}
\clearpage

\section{Second letter}
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{plot2.pdf}
\caption{plot2.pdf}
\label{fig:plot2.pdf}
\end{figure}
\clearpage

\section{Discussion}
We see that both the first and second letter distributions are very skewed. To make a note of this for posterity, we can write up our discovery in a text file that we store in the reports directory. Like the graphs directory, the reports directory is generated by ProjectTemplate automatically when we run create.project(). It's meant to contain the sort of written descriptions of the results of your analyses that you'd publish in a scientific paper.

\section{Conclusion}
With that report written and stored in the reports directory, we've gone through the simplest sort of analysis you might run with ProjectTemplate

\section{References}

\end{document}
#+end_src
* Produce final report
#+name:final report
#+begin_src R :session *R* :tangle analysis/go.r :eval no
# now run LaTeX on the file in reports/letters.tex
#+end_src
* Personalised project management directories

\section{Personalised project management directories}
#+name:additions
#+begin_src R :session *R* :tangle init.r :exports none :eval no
  ####
  # init additional directories for project management
  analysisTemplate <- function(rootdir = getwd()){
   setwd(rootdir)
   # first dir
   dir.create(file.path(rootdir,'analysis'))
   dir.create(file.path(rootdir,'analysis','reports'))
   dir.create(file.path(rootdir,'data'))
   dir.create(file.path(rootdir,'document'))
   dir.create(file.path(rootdir,'metadata'))
   dir.create(file.path(rootdir,'references'))
   # dir.create(file.path(rootdir,'tools'))
   # dir.create(file.path(rootdir,'versions'))
   dir.create(file.path(rootdir,'admin'))
   dir.create(file.path(rootdir,'admin','proposal'))
   dir.create(file.path(rootdir,'admin','budget'))
   file.create(file.path(rootdir,'admin','DataManagementPlan.txt'))
   }

#+end_src


#+begin_src R :session *R* :tangle init.r :eval no
  ####
  # init additional directories for project management
  analysisTemplate()
#+end_src
#+begin_src R :session *R* :tangle init.r :eval no
  dir()
#+end_src

| admin                   |
| analysis                |
| data                    |
| document                |
| init.r                  |
| metadata                |
| ProjectTemplateDemo.org |
| references              |
| tools                   |
| versions                |
* Navigating using other code editors
\section{Navigating using other code editors}
Emacs is not for everyone. 
#+begin_quote
a great operating system, lacking only a decent editor
#+end_quote
http://upsilon.cc/~zack/blog/posts/2008/10/from_Vim_to_Emacs_-_part_1/


* RStudio

Let's take a look at the project using RStudio.

# because an aim of this project is to complete the setup on a completely fresh machine from the single compendium file, 
# the screen shot from my Rstudio session will be lacking and break the latex code.
# I've commented it out here.
#\begin{figure}[!h]
#\centering
#\includegraphics[width=1.3\textwidth]{Rstudio.pdf}
#\caption{Rstudio.pdf}
#\label{fig:Rstudio.pdf}
#\end{figure}
* NMMAPSlite
\section{NMMAPSlite}

To fit a poisson GAM following the general apprach from the NMMAPS paradigm.
\begin{eqnarray*}
log(O_{i})  & = & s(temp_{i}) + s(dewpoint_{i})  \\	
& &   + s(Time, df = 7 \times YearsOfData, fixed = T) \\
& &   + offset(log(Pop_{i}))\\
\end{eqnarray*}
\noindent Where:\\\\
\indent $O_{i}$ = Counts of health outcome on $day_{i}$ \\
\indent $s(temp_{i}) + s(dewpoint_{i})$ = Non-parametric smooths of temperature and dewpoint on $day_{i}$ \\
\indent $s(Time, df = 7 \times YearsOfData, fixed = T)$ = parametric smooth of time (days) to adjust for time varying confounders \\
\indent $Pop_{i}$ = interpolated population on $day_{i}$ \\
#+name:NMMAPSliteEgGAM
#+begin_src R :session *R* :tangle analysis/src/NMMAPSliteEgGAM.r :exports none :eval no
  # func
  install.packages('NMMAPSlite')
  require(NMMAPSlite)
  initDB('data')
  install.packages('mgcv')
  require(mgcv)
  # load
  cities <- getMetaData('cities')
  head(cities)
  cit <- 'chic'
  subset(cities, city == cit)
  data <- readCity(cit)
  data$yy <- substr(data$date,1,4)
  
  # clean
  png('reports/NMMAPSliteEgGAM-qc.png', res = 150, height = 1000, width = 1000)
  par(mfrow=c(2,1))
  with(subset(data, agecat == '75p'), plot(date, tmax))
  with(subset(data, agecat == '75p'), plot(date, cvd, type ='l', col = 'grey'))
  with(subset(data, agecat == '75p'), lines(lowess(date, cvd, f = 0.015)))
  dev.off()
  
  # do
  numYears<-length(names(table(data$yy)))
  df <- subset(data, agecat == '75p')
  df$time <- as.numeric(df$date)
  fit <- gam(cvd ~ s(tmax) + s(dptp) + s(time, k= 7*numYears, fx=T), data = df, family = poisson)
  
  # report
  png('reports/NMMAPSliteEgGAM-exposureResponse.png', res = 150, height = 1000, width = 1000)
  par(mfrow=c(2,2))
  plot(fit)
  dev.off()
  
  # archive
  cache('df')
  cache('numYears')
  
#+end_src

* NMMAPSlite readCity()

# \begin{figure}[!h]
# \centering
# \includegraphics[width=.6\textwidth]{NMMAPSliteEgGAM-qc.png}
# \caption{NMMAPSliteEgGAM-qc.png}
# \label{fig:NMMAPSliteEgGAM-qc.png}
# \end{figure}
* NMMAPSlite GAM
# \begin{figure}[!h]
# \centering
# \includegraphics[width=.6\textwidth]{NMMAPSliteEgGAM-exposureResponse.png}
# \caption{NMMAPSliteEgGAM-exposureResponse.png}
# \label{fig:NMMAPSliteEgGAM-exposureResponse.png}
# \end{figure}
* NMMAPSlite ad hoc extension
\section{NMMAPSlite ad hoc extension}
So now if a user wants to do the analysis differently with the same data they can.
#+name:NMMAPSliteEgGAM
#+begin_src R :session *R* :tangle analysis/src/NMMAPSliteEgGAM-extension.r :eval no 
  require(ProjectTemplate)
  load.project()
  ls()
  # fit without seasonal cycle (df = 1)
  fit <- gam(cvd ~ s(tmax) + s(dptp) + 
    s(time, k= numYears/numYears, fx=T), 
    data = df, family = poisson)
  png('reports/NMMAPSliteEgGAM-exposureResponse-extension.png', res = 150, height = 1000, width = 1000)
  par(mfrow=c(2,2))
  plot(fit)
  dev.off()
#+end_src
* NMMAPSlite GAM extension
# \begin{figure}[!h]
# \centering
# \includegraphics[width=.6\textwidth]{NMMAPSliteEgGAM-exposureResponse-extension.png}
# \caption{NMMAPSliteEgGAM-exposureResponse-extension.png}
# \label{fig:NMMAPSliteEgGAM-exposureResponse-extension.png}
# \end{figure}
* Other case studies
** learnR
#+name:learnR.Rnw
#+begin_src R :session *R* :tangle analysis/reports/learnR.Rnw :exports none :eval no
% NB in Rstudio might have to go tools/options and tick `Invoke compiler via texi2dvi script'
\documentclass[a4paper]{article}
\usepackage{fancyhdr} %For headers and footers
\pagestyle{fancy} %For headers and footers
\usepackage{lastpage} %For getting page x of y
\usepackage{float} %Allows the figures to be positioned and formatted nicely
\floatstyle{boxed} %using this
\restylefloat{figure} %and this command
\usepackage{url} %Formatting of yrls
\usepackage{verbatim}
\usepackage{cite} 
\usepackage{hyperref} 
%Define all the headers and footers
\lhead{}
\chead{NCEPH Working Paper}
\rhead{}
\lfoot{Ivan C Hanigan}
\cfoot{\today}
\rfoot{\thepage\ of \pageref{LastPage}}
\usepackage{Sweave}
\begin{document}
\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
%\input{learnR-concordance}
\title{Example Sweave Document}
\author{Ivan C. Hanigan$^{1}$}
\date {\today}
\maketitle
\begin{itemize}
\item [$^1$] National Centre for Epidemiology and Population Health, \\Australian National University.
\end{itemize}

\setcounter{page}{1}
\pagenumbering{roman}
\tableofcontents 
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}
This is an introduction to some resources that are useful for learning R.  
\section{The R code that produced this report}
It is important to appreciate that R is free and open source software.  This means that any code you write can be viewed and modified by others.  In some cases we need to protect our Intellectual Property and the following statement is an attempt to ascribe copyright to our work, even though it remains open source.

``I support the philosophy of Reproducible Research \cite{Peng2011}, and where possible I provide data and code in the statistical software R that will allow analyses to be reproduced.  This document is prepared automatically from the associated Sweave (RNW) file.  If you do not have access to the RNW file please contact me.''
<<eval=FALSE,echo=FALSE,keep.source=TRUE>>=
cat('
 #######################################################################
 ## The R code is free software; please cite this paper as the source.  
 ## Copyright 2012, Ivan C Hanigan <ivan.hanigan@gmail.com> 
 ## This program is free software; you can redistribute it and/or modify
 ## it under the terms of the GNU General Public License as published by
 ## the Free Software Foundation; either version 2 of the License, or
 ## (at your option) any later version.
 ## 
 ## This program is distributed in the hope that it will be useful,
 ## but WITHOUT ANY WARRANTY; without even the implied warranty of
 ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 ## GNU General Public License for more details.
 ## Free Software
 ## Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
 ## 02110-1301, USA
 #######################################################################
')
@ 


\subsection{func}
I'll use the following packages:
<<eval=TRUE,echo=TRUE,keep.source=TRUE>>=  
if(!require(xtable)) install.packages('xtable', repos = 'http://cran.csiro.au')
require(xtable)
#require(ggplot2)
#require(ProjectTemplate)
@
<<eval=FALSE,echo=FALSE,keep.source=TRUE>>=  
create.project('analysis', minimal = TRUE)
dir.create('analysis/reports')
# the plan
@
\subsection{Some Code}
<<eval=TRUE,echo=TRUE,keep.source=TRUE>>=
x<-rnorm(100,10,5)
y<-rnorm(100,20,15)
fit <- lm(y~x)
summary(fit)
@
Using the xtable package allows results to be displyed in tables and has built in support for some R objects, so summrising the linear fit above in Table ~\ref{ATable}.
<<eval=TRUE,echo=FALSE,results=tex>>=
require(xtable)
xtable(fit, caption="Example Table",digits=4,table.placement="H",label="ATable")
@
\subsection{A Plot}
 
Plots intergrate easily, using the \LaTeX float package as can be seen in figure ~\ref{aPlot.png}.  However I like to make them as pngs and then include.

<<eval=TRUE,echo=FALSE,keep.source=TRUE>>=  
png('aPlot.png', res=200,width = 600, height = 600)
plot(x,y,main="Example Plot",xlab="X Variable",ylab="Y Variable")
abline(fit,col="Red")
dev.off()
@
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{aPlot.png}
\caption{aPlot.png}
\label{fig:aPlot.png}
\end{figure}
\clearpage
\section{Remembering the points}
This blog post \url{http://www.win-vector.com/blog/2012/04/how-to-remember-point-shape-codes-in-r/} says:

I suspect I am not unique in not being able to remember how to control the point shapes in R. Part of this is a documentation problem: no package ever seems to write the shapes down. All packages just use the usual set that derives from S-Plus and was carried through base-graphics, to grid, lattice and ggplot2. The quickest way out of this is to know how to generate an example plot of the shapes quickly. We show how to do this in ggplot2. This is trivial- but you get tired of not having it immediately available.


I like it but it is not as complate as the plot shown in Figure \ref{fig:pchopts.png} from the `R for Beginners' document by Emmanuel Paradis \cite{Paradis2002}.  I also find I often get disoriented using ggplot2.

<<eval=TRUE, echo=FALSE>>=
# it had to be fixed
# sum <- ggplot()
# for(i in 1:25) {
#    sum <- sum +
#       geom_point(data=data.frame(x=c(i)),aes(x=x,y=x),shape=i)
# }
# sum
# but this still doesn't work properly
# ggplot(data=data.frame(x=as.factor(1:16))) + geom_point(aes(x=x,y=x)) +
#     facet_wrap(~x,scales='free')
# I like base graphics anyway
png('pchopts.png')
par(mfrow=c(3,10), mar=c(0,0,2,0))
for(i in c(1:25)){
 plot(1,1,pch=i, axes=F, cex = 3, col = 'blue', bg = 'yellow')
 title(i)
 }
for(i in c("*", "?", ".", "X", "a")){
 plot(1,1,pch=i, axes=F, cex = 3, col = 'blue', bg = 'yellow')
 title(i)
 }
dev.off()
@
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{pchopts.png}
\caption{pchopts.png}
\label{fig:pchopts.png}
\end{figure}


\section{Conclusion}
In conclusion, sweave rocks.


\begin{thebibliography}{1}
\bibitem{Paradis2002}
Emmanuel Paradis.
\newblock {R for Beginners}.
\newblock 2002.

\bibitem{Peng2011}
Roger~D Peng.
\newblock {Reproducible research in computational science.}
\newblock {\em Science (New York, N.Y.)}, 334(6060):1226--7, December 2011.

\end{thebibliography}

\section{System State}
<<eval=TRUE,echo=TRUE,keep.source=TRUE>>=
sessionInfo()
@




\end{document}

#+end_src

** TODO introducing R objects
#+name:introducingRobjects
#+begin_src R :session *R* :tangle analysis/reports/introducingRobjects.r :exports none :eval no
  #Introduction to R objects
  # Joseph Guillaume 2009-08-13
  # https://alliance.anu.edu.au/access/content/group/4e0f55f1-b540-456a-000a-24730b59fccb/R%20Resources/Intro%20to%20R/
  "
  statistics & graphics environment
  similar functions to SPlus
  
  command window
  text editor
  change directory
  
  scripting language
  command and arguments - like arcgis but text rather than graphical
  result in an object rather than just display
          flexible
  why scripting:
          easy to redo from scratch
          full documentation of process
          as concise as possible once you understand it
  some argue it is less intuitive
          ?help
          autocomplete
          cran website
          mailing lists
  
  
  #Datasets used here
  Synthetic dataset
  timedata
          date,id,count
          2004-2007 by month
  shpfile of lines with z values
  
  #Packages
  made of many packages
  many ship with R in the first place:
          foreign
  
  others can install within it:
          rodbc
          rgdal, sp
          shapefiles
  "
  
  #########################
  #Syntax
  #########################
  #objects and assignment
  s<- -1
  s
  
  #operators
  t<- s+5
  t==s
  t
  
  #functions
  seq
  ?seq
  series<-seq(from=s,to=t,length.out=5)
  series
  
  #comments
   
  # navigating workspace
  getwd()
  setwd('~/tools/ProjectTemplateDemo/analysis/data/intro-to-r')
  dir() 
  #########################
  # Loading data
  #########################
  
  #Reading from CSV file
  # Readable in text format
  timedata<-read.csv("timedata.csv")
  
  #Reading from MS Excel spreadsheet
  # Problems with NAs
  # installing packages
  install.packages('RODBC')
  library(RODBC)
  chan<-odbcConnectExcel("timedata.xls")
  sqlTables(chan)$TABLE_NAME
  sheet<-sqlFetch(chan,"timedata")
  
  #Reading SHP dbf data file
  library(foreign)
  shpdbf<-read.dbf("3d_line.dbf")
  shpdbf
  
  #########################
  #Looking at the data
  #########################
  #The object
  timedata
  #Summarize data
  summary(timedata)
  
  #Subsetting
  #First 10 rows, all columns
  timedata[1:10,]
  #110th to 120th row, 2nd and 3rd column
  timedata[110:120,2:3]
  #Using column names
  names(timedata)
  summary(timedata$count)
  #Using boolean expressions - all rows that have count>5
  timedata[timedata$count>70,]
  #Using subset
  subset(timedata,subset=timedata$count<10,select=date)
  
  #Look at structure
  str(timedata)
  #Data frame. Table of columns of different data types
  #Usual type of object when loading data
  #Data types
  # numeric,text,date (will come back to it)
  
  #Other ways of storing data
  #named vectors
  v<-c(1,3,4,5)
  names(v)<-c("one","two","three")
  v
  
  #matrices: single data type, multiple columns & rows
  m<-matrix(1:100,ncol=10,nrow=10)
  m
  
  #List, can hold any objects of different types
  L<-list(
          vector=v,
          matrix=m,
          data.frame=timedata[1:5,]
  )
  L
  
  #Other types of objects for specific tasks, we'll see later
  
  ########################
  #Basic analyses
  ########################
  
  plot(timedata$id,timedata$count)
  plot(
          count~id,
          data=timedata,
          main="Distribution of counts by line number",
          xlab="ID",
          ylab="Count",
          col="blue",
          lwd=2,
          type="p"
          )
  ?plot
  hist(timedata$count)
  plot(density(timedata$count))
  
  boxplot(timedata$count)
  
  #png("boxplot.png")
  win.metafile("boxplot.wmf")
  boxplot(count~id,
          data=timedata,
          main="Boxplots of count by line",
          xlab="Line number",
          ylab="Count"
  )
  dev.off()
  
  #Linear regression
  linreg<-lm(count~id,data=timedata)
  linreg
  summary(linreg)
  
  
  ########################
  #Time series analysis
  ########################
  
  #Convert between data types
  # as.numeric, as.data.frame, as.matrix, as.character etc.
  
  timedata$date<-as.Date(timedata$date)
  timedata$year<-as.numeric(format(timedata$date,"%Y"))
  str(timedata)
  
  ?ts
  timedata_ts<-ts(data=timedata$count[timedata$id==5],
          start=min(timedata$year),
          freq=12
  )
  
  timedata_ts
  plot(timedata_ts)
  
  #Loess decomposition
  s5l<-stl(timedata_ts,s.window=3)
  plot(s5l)
  #Periodic decomposition
  s5p<-stl(timedata_ts,s.window="periodic")
  plot(s5p)
  
  #Show autocorrelation of remainder
  acf(s5l$time.series[,"remainder"])
  
  
  ########################
  # More complex tasks:
  # Programming concepts
  ########################
  #Doing stl decompositions for every the time series of every even numbered line
  
  #Rather than repeating code for each time series separately, we can use programming concepts:
  # Functions
  # Loops - for loops
  # Conditional branching - if statements
  
  #Functions
  #Grouping code that is frequently reused
  plot_ts_stl<-function(id){
          timedata_ts<-ts(data=timedata$count[timedata$id==id],
                  start=2004,
                  freq=12
          )
          s<-stl(timedata_ts,s.window=3)
          plot(s,main=sprintf("STL decomposition of line %d",id))
  }
  
  par(ask=TRUE)
  #Loop
  for (id in 1:10){
          #Conditional
          if (id %% 2==0) plot_ts_stl(id)
  }
  
  
  ########################
  # Spatial data
  ########################
  
  library(rgdal)
  
  #Creating a line from scratch
  x<-1:10
  lines<-SpatialLines(list(
          Lines(list(
                  Line(matrix(c(x,x),ncol=2))
                  ),ID=c("x1")
          ),
          Lines(list(
                  Line(matrix(c(x,x+1),ncol=2))
                  ),ID=c("x2")
          )
          )#List of Lines
  )
  lines
  data<-data.frame(ID=c(1,2))
  row.names(data)<-c("x1","x2")
  
  a<-SpatialLinesDataFrame(lines,data)
  writeOGR(a,"lines.shp","lines","ESRI Shapefile")
  
  b<-readOGR("lines.shp","lines")
  identical(coordinates(a),coordinates(b))
  data.frame(a)
  data.frame(b)
  
  ########################
  # Example application - splitting lines at crests
  ########################
  
  plot_breaks<-function(z,return.which=TRUE,debug=TRUE){
          wanted<-2:(length(z)-1)
          #find vertices that are greater than those on either side
          localmaxes<-which(sapply(wanted,function(i) z[i]>=z[i-1] & z[i]>=z[i+1]))
  
          #Z values of local maximums
          if (debug) print(z[wanted][localmaxes])
  
          #Split line
          breaks<-unique(c(1,localmaxes+1,length(z)))
          aa<-rep(NA,length(z))
          for (i in 2:length(breaks)) {
                  aa[breaks[i-1]:breaks[i]]<-i
          }
  
          if (debug) plot(z,col=aa,type="b")
          if (return.which) return(split(1:length(z),aa))
          else return(split(z,aa))
  
  }#plot_breaks
  
  
  #Two parabolas
  z<-c(50-(x-5)**2,30-(x-3)**2)
  plot_breaks(z)
  
  #Random walk
  z<-rep(NA,100)
  z[1]<-10
  for (i in 2:100){
          z[i]<-z[i-1]+runif(min=-5,max=5,n=1)
  }
  plot_breaks(z)
  
  
  ######################################
  # With an actual 3d line shapefile
  ######################################
  
  library(shapefiles)
  line3d_coords<-read.shp("3d_line.shp")
  str(line3d_coords)
  
  lines<-lapply(line3d_coords$shp,function(x) x$points)
  lines
  
  final_data<-NULL
  final_lines<-list()
  for (i in 1:length(lines)){
  
          coords<-lines[[i]] #coordinates(lines[[i]])
          origid<-as.character(i)
          linesplit<-plot_breaks(coords[,3],debug=FALSE)
  
          #Add next coordinate to all split lines
          for (i in 2:length(linesplit)){
                  linesplit[[i-1]]<-c(linesplit[[i-1]],linesplit[[i]][1])
          }
  
          newlines<-lapply(linesplit,function(x) coords[x,])
          newlines<-lapply(1:length(newlines),function(x) Lines(list(Line(newlines[[x]][,1:2])),ID=sprintf("x%s.%s",origid,x)))
          ids<-sapply(1:length(newlines),function(x) sprintf("x%s.%s",origid,x))
  
          data<-data.frame(origID=rep(1,length(newlines)),newID=ids)
          row.names(data)<-ids
  
          final_lines<-c(final_lines,newlines)
          final_data<-rbind(final_data,data)
  
  }#lines
  final<-SpatialLinesDataFrame(SpatialLines(final_lines),final_data)
  writeOGR(final,"split_lines.shp","lines","ESRI Shapefile")
  
  #Output in KML (e.g. for google earth)
  writeOGR(final,"split_lines.kml","lines","KML")
  
  
  ########################
  # Resources
  ########################
  http://cran.r-project.org/
  
  http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf
  http://cran.r-project.org/doc/contrib/Short-refcard.pdf
  http://cran.r-project.org/web/views/
  http://cran.r-project.org/web/views/TimeSeries.html
  http://cran.r-project.org/web/views/Spatial.html
  
  http://geodacenter.asu.edu/r-spatial-projects
  http://www.bostongis.com/
  
#+end_src

* Conclusions
The Emacs Orgmode file is the compendium from which the whole analysis can be re-created.
The upshot is that once I have developed the project's main \emph{\textbf{.org}} file and completed the analysis I can send it (and only it) to another analyst and if they run it (using Emacs) it should get the project to exactly the same state that it was in when I left it, ready for reproduction or extension.
-------
\bigskip
THANKS for listening!

To see a copy of the org file for this demo go to https://github.com/ivanhanigan/ProjectTemplateDemo
# * Why not use make?
# My main reason for not using this useful approach is that I work with other people, who may not want to `play' with as many software tools as I do.
# The system I use involves R, Emacs orgmode, ESS, LaTeX (and optionally git).  
# The end user of my work may not want to use any of these (apart from R), so I don't want to include too many extra things.
* References
\section{References}
# \bibliographystyle{unsrt}
# \bibliography{I:/references/library}

\begin{thebibliography}{1}

\bibitem{Gentleman2007}
Robert Gentleman and Duncan {Temple Lang}.
\newblock {Statistical Analyses and Reproducible Research}.
\newblock {\em Journal of Computational and Graphical Statistics}, 16(1):1--23,
  March 2007.

\bibitem{Schulte}
E~Schulte, D~Davison, T~Dye, and C~Dominik.
\newblock {A Multi-Language Computing Environment for Literate Programming and
  Reproducible Research}.
\newblock {\em Journal of Statistical Software}, 46(3), 2012.

\bibitem{Peng2004}
RD~Peng and LJ~Welty.
\newblock {The NMMAPSdata Package}.
\newblock {\em R News}, 4(2):10--14, 2004.

\end{thebibliography}
# notes for presentation
# have open 
# a complex flow diagram to make the point about 'very' complex projects
# the ppt
# c:/temp/ptdemo
# 	ask if keen to see live demo
# Rstudio (empty) 

# intro with vcomplex flow diag
# explain the theoretical background of the analysis (indirect, direct and regression adjustment)
# goto ppt (mention Jermy anglims presentation)
# talk to slides till dir()
# flip into orgmode and show 6 steps to create, load and do project
# do the init.r and the second tangle
# back to ppt and go to the report latex bit
# go to the orgmode and run the go and show the latex (optionally run that)
# back to ppt and show personalised
# back to orgmode to show analysisTemplate() function
# back to ppt to show 'emacs not for everyone'
# goto Rstudio and create proj, setup git, 
# back to ppt to conclude and show link to github and recommend the references

* System State
\section{System State}
Note down the state of the computer at the time of the successful run (note that this doesn't export to the LaTeX file using exports results).
#+name:sessionInfo
#+begin_src R :session *R*  :eval no 
sessionInfo()
#+end_src
